<!DOCTYPE html>
<html lang="en-US">
<!--**************************************-->
<!--*    Generated from PreTeXt source   *-->
<!--*    on 2020-11-13T12:10:24+01:00    *-->
<!--*                                    *-->
<!--*      https://pretextbook.org       *-->
<!--*                                    *-->
<!--**************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body>
<h6 class="heading"><span class="type">Paragraph</span></h6>
<p>Step <a class="xref" data-knowl="./knowl/maxalg-3-centralizer.html" title="Item 3">3</a> is another straightforward linear algebra computation. In step <a class="xref" data-knowl="./knowl/maxalg-4-ad-rep.html" title="Item 4">4</a>, the key observation is that any linear map \(A\in C(\mathfrak{t})\) preserves the eigenspaces of all the derivations \(\delta\in\mathfrak{t}\text{.}\) Hence such a linear map \(A\) also preserves the layers \(V_\lambda\) of the \(F^k\)-grading. It follows that each map \(\ad(A)\) restricts to a linear map \(\ad(A)\colon \mathfrak{gl}(V_\lambda)\to \mathfrak{gl}(V_\lambda)\) for each weight \(\lambda\text{.}\) The direct sum of these representations gives the representation \(\ad\colon C(\mathfrak{t})\to \bigoplus_{\lambda}\mathfrak{gl}(\mathfrak{gl}(V_\lambda))\text{.}\)</p>
<span class="incontext"><a href="section-constructions.html#p-122">in-context</a></span>
</body>
</html>
